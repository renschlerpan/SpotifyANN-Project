{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load data\n",
    "spotify_data = pd.read_csv('cleaned_spotify_data.csv').sample(frac=0.25, random_state=42)\n",
    "user_profiles = pd.read_csv('cleaned_user_profiles.csv').sample(frac=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "categorical_features = ['genre', 'artist', 'mode']  # mode as categorical if it represents major/minor\n",
    "numerical_features = ['danceability', 'energy', 'key', 'loudness', 'speechiness', 'acousticness', \n",
    "                      'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms']\n",
    "\n",
    "# Handling missing values for numerical features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with mean\n",
    "    ('scaler', StandardScaler())  # Scale features\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Adjust the categorical transformer to include dimensionality reduction\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ('svd', TruncatedSVD(n_components=50))  # Reduce dimensions to 50\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "spotify_features_scaled = preprocessor.fit_transform(spotify_data)\n",
    "user_features_scaled = preprocessor.transform(user_profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_features_scaled = preprocessor.fit_transform(spotify_data)\n",
    "user_features_scaled = preprocessor.transform(user_profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8208, 61), (3236, 61))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spotify_features_scaled.shape, user_features_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genre columns: ['genre_edm', 'genre_latin', 'genre_pop', 'genre_r&b', 'genre_rap', 'genre_rock']\n",
      "Sample artist columns: ['artist_!deladap', 'artist_$ANFI', 'artist_$IFRA', 'artist_$uicideBoy$', 'artist_*NSYNC']\n"
     ]
    }
   ],
   "source": [
    "# Fetching new column names for categorical features\n",
    "genre_columns = [col for col in preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out() if 'genre' in col]\n",
    "artist_columns = [col for col in preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out() if 'artist' in col]\n",
    "\n",
    "print(\"Genre columns:\", genre_columns)\n",
    "print(\"Sample artist columns:\", artist_columns[:5])  # Displaying first few artist columns to keep the output manageable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gc  # Import garbage collector\n",
    "\n",
    "def calculate_interaction_vectorized_batched(user_features_scaled, spotify_features_scaled, user_profiles, spotify_data, batch_size=100):\n",
    "    user_features_scaled = user_features_scaled.astype(np.float32)\n",
    "    spotify_features_scaled = spotify_features_scaled.astype(np.float32)\n",
    "\n",
    "    num_users = user_features_scaled.shape[0]\n",
    "    num_songs = spotify_features_scaled.shape[0]\n",
    "    interaction_scores = np.zeros((num_users, num_songs), dtype=np.float32)\n",
    "    \n",
    "    # Define the genre and artist column indices\n",
    "    genre_indices = np.array([user_profiles.columns.get_loc(name) for name in genre_columns])\n",
    "    artist_indices = np.array([user_profiles.columns.get_loc(name) for name in artist_columns])\n",
    "\n",
    "    for start in range(0, num_users, batch_size):\n",
    "        end = min(start + batch_size, num_users)\n",
    "        batch_user_features = user_features_scaled[start:end]\n",
    "        print(f\"Processing user batch: {start//batch_size+1}/{(num_users-1)//batch_size+1}, Users {start}-{end-1}\")\n",
    "        for start_song in range(0, num_songs, batch_size):\n",
    "            end_song = min(start_song + batch_size, num_songs)\n",
    "            batch_spotify_features = spotify_features_scaled[start_song:end_song]\n",
    "\n",
    "            distances = np.sqrt(((batch_user_features[:, np.newaxis, :] - batch_spotify_features[np.newaxis, :, :]) ** 2).sum(axis=2))\n",
    "            feature_similarity = np.exp(-distances).astype(np.float32)\n",
    "\n",
    "            genre_similarity = np.equal(user_profiles.iloc[start:end, genre_indices].values[:, np.newaxis, :], \n",
    "                                        spotify_data.iloc[start_song:end_song, genre_indices].values[np.newaxis, :, :]).astype(np.float32).mean(axis=2)\n",
    "            artist_similarity = np.equal(user_profiles.iloc[start:end, artist_indices].values[:, np.newaxis, :], \n",
    "                                         spotify_data.iloc[start_song:end_song, artist_indices].values[np.newaxis, :, :]).astype(np.float32).mean(axis=2)\n",
    "\n",
    "            batch_interaction_scores = 0.4 * feature_similarity + 0.5 * genre_similarity + 0.1 * artist_similarity\n",
    "            interaction_scores[start:end, start_song:end_song] = batch_interaction_scores\n",
    "\n",
    "            # Optional: Clear memory of temporary variables\n",
    "            del distances, feature_similarity, genre_similarity, artist_similarity\n",
    "            gc.collect()\n",
    "\n",
    "    return interaction_scores\n",
    "\n",
    "# def calculate_interaction_vectorized_batched(user_features_scaled, spotify_features_scaled, user_profiles, spotify_data, batch_size=100):\n",
    "#     # Define data as float32 to save memory\n",
    "#     user_features_scaled = user_features_scaled.astype(np.float32)\n",
    "#     spotify_features_scaled = spotify_features_scaled.astype(np.float32)\n",
    "\n",
    "#     num_users = user_features_scaled.shape[0]\n",
    "#     num_songs = spotify_features_scaled.shape[0]\n",
    "#     interaction_scores = np.zeros((num_users, num_songs), dtype=np.float32)  # Use float32 for interaction scores\n",
    "    \n",
    "#     for start in range(0, num_users, batch_size):\n",
    "#         end = min(start + batch_size, num_users)\n",
    "#         batch_user_features = user_features_scaled[start:end]\n",
    "#         print(f\"Processing user batch: {start//batch_size+1}/{(num_users-1)//batch_size+1}, Users {start}-{end-1}\")\n",
    "#         for start_song in range(0, num_songs, batch_size):\n",
    "#             end_song = min(start_song + batch_size, num_songs)\n",
    "#             batch_spotify_features = spotify_features_scaled[start_song:end_song]\n",
    "\n",
    "#             distances = np.sqrt(((batch_user_features[:, np.newaxis, :] - batch_spotify_features[np.newaxis, :, :]) ** 2).sum(axis=2))\n",
    "#             feature_similarity = np.exp(-distances).astype(np.float32)  # Convert to float32 immediately\n",
    "\n",
    "#             genre_similarity = np.equal(user_profiles['genre'].values[start:end][:, np.newaxis], spotify_data['genre'].values[start_song:end_song][np.newaxis, :]).astype(np.float32)\n",
    "#             artist_similarity = np.equal(user_profiles['artist'].values[start:end][:, np.newaxis], spotify_data['artist'].values[start_song:end_song][np.newaxis, :]).astype(np.float32)\n",
    "\n",
    "#             batch_interaction_scores = 0.4 * feature_similarity + 0.5 * genre_similarity + 0.1 * artist_similarity\n",
    "#             interaction_scores[start:end, start_song:end_song] = batch_interaction_scores\n",
    "\n",
    "#             # Optional: Clear memory of temporary variables\n",
    "#             del distances, feature_similarity, genre_similarity, artist_similarity\n",
    "#             gc.collect()\n",
    "\n",
    "#     return interaction_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing user batch: 1/33, Users 0-99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing user batch: 2/33, Users 100-199\n",
      "Processing user batch: 3/33, Users 200-299\n",
      "Processing user batch: 4/33, Users 300-399\n",
      "Processing user batch: 5/33, Users 400-499\n",
      "Processing user batch: 6/33, Users 500-599\n",
      "Processing user batch: 7/33, Users 600-699\n",
      "Processing user batch: 8/33, Users 700-799\n",
      "Processing user batch: 9/33, Users 800-899\n",
      "Processing user batch: 10/33, Users 900-999\n",
      "Processing user batch: 11/33, Users 1000-1099\n",
      "Processing user batch: 12/33, Users 1100-1199\n",
      "Processing user batch: 13/33, Users 1200-1299\n",
      "Processing user batch: 14/33, Users 1300-1399\n",
      "Processing user batch: 15/33, Users 1400-1499\n",
      "Processing user batch: 16/33, Users 1500-1599\n",
      "Processing user batch: 17/33, Users 1600-1699\n",
      "Processing user batch: 18/33, Users 1700-1799\n",
      "Processing user batch: 19/33, Users 1800-1899\n",
      "Processing user batch: 20/33, Users 1900-1999\n",
      "Processing user batch: 21/33, Users 2000-2099\n",
      "Processing user batch: 22/33, Users 2100-2199\n",
      "Processing user batch: 23/33, Users 2200-2299\n",
      "Processing user batch: 24/33, Users 2300-2399\n",
      "Processing user batch: 25/33, Users 2400-2499\n",
      "Processing user batch: 26/33, Users 2500-2599\n",
      "Processing user batch: 27/33, Users 2600-2699\n",
      "Processing user batch: 28/33, Users 2700-2799\n",
      "Processing user batch: 29/33, Users 2800-2899\n",
      "Processing user batch: 30/33, Users 2900-2999\n",
      "Processing user batch: 31/33, Users 3000-3099\n",
      "Processing user batch: 32/33, Users 3100-3199\n",
      "Processing user batch: 33/33, Users 3200-3235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((3236, 8208), 0.016233598813414574)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert user_profiles and spotify_data back to DataFrame for genre and artist columns\n",
    "user_profiles = pd.DataFrame(user_profiles, columns=preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out())\n",
    "spotify_data = pd.DataFrame(spotify_data, columns=preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out())\n",
    "\n",
    "# Execute interaction calculations\n",
    "interaction_scores = calculate_interaction_vectorized_batched(user_features_scaled, spotify_features_scaled, user_profiles, spotify_data)\n",
    "\n",
    "# Compute interaction threshold\n",
    "interaction_threshold = np.percentile(interaction_scores, 75)  # top 25% as positive interaction\n",
    "\n",
    "# Calculate interaction\n",
    "interaction = (interaction_scores >= interaction_threshold).astype(int)\n",
    "\n",
    "# Flatten interaction matrix and features for neural network input\n",
    "X = spotify_features_scaled.repeat(len(user_profiles), axis=0)\n",
    "y = interaction.flatten()\n",
    "\n",
    "interaction_scores.shape, interaction_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Student\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled.\n",
      "Data split into training and testing sets. Training model...\n",
      "Epoch 1/10\n",
      "\u001b[1m664028/664028\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1345s\u001b[0m 2ms/step - accuracy: 0.7499 - loss: 0.5592 - val_accuracy: 0.7503 - val_loss: 0.5526\n",
      "Epoch 2/10\n",
      "\u001b[1m664028/664028\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1368s\u001b[0m 2ms/step - accuracy: 0.7498 - loss: 0.5542 - val_accuracy: 0.7502 - val_loss: 0.5509\n",
      "Epoch 3/10\n",
      "\u001b[1m664028/664028\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1407s\u001b[0m 2ms/step - accuracy: 0.7499 - loss: 0.5533 - val_accuracy: 0.7503 - val_loss: 0.5507\n",
      "Epoch 4/10\n",
      "\u001b[1m664028/664028\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1311s\u001b[0m 2ms/step - accuracy: 0.7496 - loss: 0.5533 - val_accuracy: 0.7502 - val_loss: 0.5506\n",
      "Epoch 5/10\n",
      "\u001b[1m664028/664028\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1139s\u001b[0m 2ms/step - accuracy: 0.7499 - loss: 0.5527 - val_accuracy: 0.7503 - val_loss: 0.5509\n",
      "Epoch 6/10\n",
      "\u001b[1m664028/664028\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1158s\u001b[0m 2ms/step - accuracy: 0.7498 - loss: 0.5528 - val_accuracy: 0.7503 - val_loss: 0.5500\n",
      "Epoch 7/10\n",
      "\u001b[1m664028/664028\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1146s\u001b[0m 2ms/step - accuracy: 0.7500 - loss: 0.5525 - val_accuracy: 0.7503 - val_loss: 0.5503\n",
      "Epoch 8/10\n",
      "\u001b[1m664028/664028\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1155s\u001b[0m 2ms/step - accuracy: 0.7499 - loss: 0.5526 - val_accuracy: 0.7503 - val_loss: 0.5500\n",
      "Epoch 9/10\n",
      "\u001b[1m664028/664028\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1285s\u001b[0m 2ms/step - accuracy: 0.7499 - loss: 0.5525 - val_accuracy: 0.7503 - val_loss: 0.5508\n",
      "Epoch 10/10\n",
      "\u001b[1m664028/664028\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1335s\u001b[0m 2ms/step - accuracy: 0.7499 - loss: 0.5526 - val_accuracy: 0.7502 - val_loss: 0.5501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "712"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "# Assuming X, y, and spotify_features_scaled are properly computed as discussed\n",
    "\n",
    "# Neural network setup with Dropout\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_dim=X.shape[1]),  # Updated to match feature dimension\n",
    "    Dropout(0.1),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model compiled.\")\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Data split into training and testing sets. Training model...\")\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 973us/step\n",
      "Prediction complete and memory cleared.\n"
     ]
    }
   ],
   "source": [
    "predicted_scores = model.predict(spotify_features_scaled).flatten()\n",
    "spotify_data['predicted_interaction'] = predicted_scores\n",
    "gc.collect()  # Clear memory of no longer needed large objects\n",
    "print(\"Prediction complete and memory cleared.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 4438)) while a minimum of 1 is required by NearestNeighbors.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m spotify_data \u001b[38;5;241m=\u001b[39m spotify_data\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39mfeatures_with_score)\n\u001b[0;32m      5\u001b[0m knn \u001b[38;5;241m=\u001b[39m NearestNeighbors(n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mknn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspotify_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures_with_score\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk-NN model set up.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m user_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m  \u001b[38;5;66;03m# Adjust based on user profile index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Student\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Student\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neighbors\\_unsupervised.py:175\u001b[0m, in \u001b[0;36mNearestNeighbors.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# NearestNeighbors.metric is not validated yet\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    157\u001b[0m )\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    159\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the nearest neighbors estimator from the training dataset.\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \n\u001b[0;32m    161\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;124;03m        The fitted nearest neighbors estimator.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Student\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:518\u001b[0m, in \u001b[0;36mNeighborsBase._fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, (KDTree, BallTree, NeighborsBase)):\n\u001b[1;32m--> 518\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_algorithm_metric()\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Student\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\Student\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1072\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1070\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[0;32m   1071\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[1;32m-> 1072\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1073\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1074\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1075\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m   1076\u001b[0m         )\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1079\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 4438)) while a minimum of 1 is required by NearestNeighbors."
     ]
    }
   ],
   "source": [
    "# k-NN model using enhanced features\n",
    "features_with_score = np.concatenate([genre_columns, artist_columns, ['predicted_interaction']])\n",
    "#drop nan values\n",
    "spotify_data = spotify_data.dropna(subset=features_with_score)\n",
    "knn = NearestNeighbors(n_neighbors=5, metric='euclidean')\n",
    "knn.fit(spotify_data[features_with_score])\n",
    "print(\"k-NN model set up.\")\n",
    "\n",
    "user_index = 3  # Adjust based on user profile index\n",
    "user_id = user_profiles.iloc[user_index]['userid']\n",
    "user_top_genre = user_profiles.iloc[user_index]['genre']\n",
    "user_feature_vector = user_features_scaled[user_index].reshape(1, -1)\n",
    "user_predicted_score = model.predict(user_feature_vector).flatten()[0]\n",
    "query_vector = np.append(user_feature_vector, user_predicted_score).reshape(1, -1)\n",
    "\n",
    "# Finding top 5 nearest songs\n",
    "distances, indices = knn.kneighbors(query_vector)\n",
    "recommended_songs = spotify_data.iloc[indices[0]]\n",
    "\n",
    "# Output recommended songs\n",
    "print(f\"Recommended Songs for User: {user_id}, Top Genre: {user_top_genre}\")\n",
    "print(recommended_songs[['track', 'artist', 'genre']])\n",
    "\n",
    "# Validation\n",
    "predicted_interactions = model.predict(X_test).flatten()\n",
    "rmse = mean_squared_error(y_test, predicted_interactions, squared=False)\n",
    "print(\"RMSE for neural network predictions:\", rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
