{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load data\n",
    "spotify_data = pd.read_csv('cleaned_spotify_data.csv').sample(frac=0.1, random_state=42)\n",
    "user_profiles = pd.read_csv('cleaned_user_profiles.csv').sample(frac=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "categorical_features = ['genre', 'artist', 'mode']  # mode as categorical if it represents major/minor\n",
    "numerical_features = ['danceability', 'energy', 'key', 'loudness', 'speechiness', 'acousticness', \n",
    "                      'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms']\n",
    "\n",
    "# Handling missing values for numerical features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with mean\n",
    "    ('scaler', StandardScaler())  # Scale features\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Adjust the categorical transformer to include dimensionality reduction\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ('svd', TruncatedSVD(n_components=50))  # Reduce dimensions to 50\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "spotify_features_scaled = preprocessor.fit_transform(spotify_data)\n",
    "user_features_scaled = preprocessor.transform(user_profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_features_scaled = preprocessor.fit_transform(spotify_data)\n",
    "user_features_scaled = preprocessor.transform(user_profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3283, 61), (3236, 61))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spotify_features_scaled.shape, user_features_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genre columns: ['genre_edm', 'genre_latin', 'genre_pop', 'genre_r&b', 'genre_rap', 'genre_rock']\n",
      "Sample artist columns: ['artist_!deladap', 'artist_$ANFI', 'artist_$IFRA', 'artist_$uicideBoy$', 'artist_-M-']\n"
     ]
    }
   ],
   "source": [
    "# Fetching new column names for categorical features\n",
    "genre_columns = [col for col in preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out() if 'genre' in col]\n",
    "artist_columns = [col for col in preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out() if 'artist' in col]\n",
    "\n",
    "print(\"Genre columns:\", genre_columns)\n",
    "print(\"Sample artist columns:\", artist_columns[:5])  # Displaying first few artist columns to keep the output manageable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gc  # Import garbage collector\n",
    "\n",
    "def calculate_interaction_vectorized_batched(user_features_scaled, spotify_features_scaled, user_profiles, spotify_data, batch_size=100):\n",
    "    user_features_scaled = user_features_scaled.astype(np.float32)\n",
    "    spotify_features_scaled = spotify_features_scaled.astype(np.float32)\n",
    "\n",
    "    num_users = user_features_scaled.shape[0]\n",
    "    num_songs = spotify_features_scaled.shape[0]\n",
    "    interaction_scores = np.zeros((num_users, num_songs), dtype=np.float32)\n",
    "    \n",
    "    # Define the genre and artist column indices\n",
    "    genre_indices = np.array([user_profiles.columns.get_loc(name) for name in genre_columns])\n",
    "    artist_indices = np.array([user_profiles.columns.get_loc(name) for name in artist_columns])\n",
    "\n",
    "    for start in range(0, num_users, batch_size):\n",
    "        end = min(start + batch_size, num_users)\n",
    "        batch_user_features = user_features_scaled[start:end]\n",
    "        print(f\"Processing user batch: {start//batch_size+1}/{(num_users-1)//batch_size+1}, Users {start}-{end-1}\")\n",
    "        for start_song in range(0, num_songs, batch_size):\n",
    "            end_song = min(start_song + batch_size, num_songs)\n",
    "            batch_spotify_features = spotify_features_scaled[start_song:end_song]\n",
    "\n",
    "            distances = np.sqrt(((batch_user_features[:, np.newaxis, :] - batch_spotify_features[np.newaxis, :, :]) ** 2).sum(axis=2))\n",
    "            feature_similarity = np.exp(-distances).astype(np.float32)\n",
    "\n",
    "            genre_similarity = np.equal(user_profiles.iloc[start:end, genre_indices].values[:, np.newaxis, :], \n",
    "                                        spotify_data.iloc[start_song:end_song, genre_indices].values[np.newaxis, :, :]).astype(np.float32).mean(axis=2)\n",
    "            artist_similarity = np.equal(user_profiles.iloc[start:end, artist_indices].values[:, np.newaxis, :], \n",
    "                                         spotify_data.iloc[start_song:end_song, artist_indices].values[np.newaxis, :, :]).astype(np.float32).mean(axis=2)\n",
    "\n",
    "            batch_interaction_scores = 0.4 * feature_similarity + 0.5 * genre_similarity + 0.1 * artist_similarity\n",
    "            interaction_scores[start:end, start_song:end_song] = batch_interaction_scores\n",
    "\n",
    "            # Optional: Clear memory of temporary variables\n",
    "            del distances, feature_similarity, genre_similarity, artist_similarity\n",
    "            gc.collect()\n",
    "\n",
    "    return interaction_scores\n",
    "\n",
    "# def calculate_interaction_vectorized_batched(user_features_scaled, spotify_features_scaled, user_profiles, spotify_data, batch_size=100):\n",
    "#     # Define data as float32 to save memory\n",
    "#     user_features_scaled = user_features_scaled.astype(np.float32)\n",
    "#     spotify_features_scaled = spotify_features_scaled.astype(np.float32)\n",
    "\n",
    "#     num_users = user_features_scaled.shape[0]\n",
    "#     num_songs = spotify_features_scaled.shape[0]\n",
    "#     interaction_scores = np.zeros((num_users, num_songs), dtype=np.float32)  # Use float32 for interaction scores\n",
    "    \n",
    "#     for start in range(0, num_users, batch_size):\n",
    "#         end = min(start + batch_size, num_users)\n",
    "#         batch_user_features = user_features_scaled[start:end]\n",
    "#         print(f\"Processing user batch: {start//batch_size+1}/{(num_users-1)//batch_size+1}, Users {start}-{end-1}\")\n",
    "#         for start_song in range(0, num_songs, batch_size):\n",
    "#             end_song = min(start_song + batch_size, num_songs)\n",
    "#             batch_spotify_features = spotify_features_scaled[start_song:end_song]\n",
    "\n",
    "#             distances = np.sqrt(((batch_user_features[:, np.newaxis, :] - batch_spotify_features[np.newaxis, :, :]) ** 2).sum(axis=2))\n",
    "#             feature_similarity = np.exp(-distances).astype(np.float32)  # Convert to float32 immediately\n",
    "\n",
    "#             genre_similarity = np.equal(user_profiles['genre'].values[start:end][:, np.newaxis], spotify_data['genre'].values[start_song:end_song][np.newaxis, :]).astype(np.float32)\n",
    "#             artist_similarity = np.equal(user_profiles['artist'].values[start:end][:, np.newaxis], spotify_data['artist'].values[start_song:end_song][np.newaxis, :]).astype(np.float32)\n",
    "\n",
    "#             batch_interaction_scores = 0.4 * feature_similarity + 0.5 * genre_similarity + 0.1 * artist_similarity\n",
    "#             interaction_scores[start:end, start_song:end_song] = batch_interaction_scores\n",
    "\n",
    "#             # Optional: Clear memory of temporary variables\n",
    "#             del distances, feature_similarity, genre_similarity, artist_similarity\n",
    "#             gc.collect()\n",
    "\n",
    "#     return interaction_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing user batch: 1/33, Users 0-99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing user batch: 2/33, Users 100-199\n",
      "Processing user batch: 3/33, Users 200-299\n",
      "Processing user batch: 4/33, Users 300-399\n",
      "Processing user batch: 5/33, Users 400-499\n",
      "Processing user batch: 6/33, Users 500-599\n",
      "Processing user batch: 7/33, Users 600-699\n",
      "Processing user batch: 8/33, Users 700-799\n",
      "Processing user batch: 9/33, Users 800-899\n",
      "Processing user batch: 10/33, Users 900-999\n",
      "Processing user batch: 11/33, Users 1000-1099\n",
      "Processing user batch: 12/33, Users 1100-1199\n",
      "Processing user batch: 13/33, Users 1200-1299\n",
      "Processing user batch: 14/33, Users 1300-1399\n",
      "Processing user batch: 15/33, Users 1400-1499\n",
      "Processing user batch: 16/33, Users 1500-1599\n",
      "Processing user batch: 17/33, Users 1600-1699\n",
      "Processing user batch: 18/33, Users 1700-1799\n",
      "Processing user batch: 19/33, Users 1800-1899\n",
      "Processing user batch: 20/33, Users 1900-1999\n",
      "Processing user batch: 21/33, Users 2000-2099\n",
      "Processing user batch: 22/33, Users 2100-2199\n",
      "Processing user batch: 23/33, Users 2200-2299\n",
      "Processing user batch: 24/33, Users 2300-2399\n",
      "Processing user batch: 25/33, Users 2400-2499\n",
      "Processing user batch: 26/33, Users 2500-2599\n",
      "Processing user batch: 27/33, Users 2600-2699\n",
      "Processing user batch: 28/33, Users 2700-2799\n",
      "Processing user batch: 29/33, Users 2800-2899\n",
      "Processing user batch: 30/33, Users 2900-2999\n",
      "Processing user batch: 31/33, Users 3000-3099\n",
      "Processing user batch: 32/33, Users 3100-3199\n",
      "Processing user batch: 33/33, Users 3200-3235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((3236, 3283), 0.01602406334131956)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert user_profiles and spotify_data back to DataFrame for genre and artist columns\n",
    "user_profiles = pd.DataFrame(user_profiles, columns=preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out())\n",
    "spotify_data = pd.DataFrame(spotify_data, columns=preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out())\n",
    "\n",
    "# Execute interaction calculations\n",
    "interaction_scores = calculate_interaction_vectorized_batched(user_features_scaled, spotify_features_scaled, user_profiles, spotify_data)\n",
    "\n",
    "# Compute interaction threshold\n",
    "interaction_threshold = np.percentile(interaction_scores, 75)  # top 25% as positive interaction\n",
    "\n",
    "# Calculate interaction\n",
    "interaction = (interaction_scores >= interaction_threshold).astype(int)\n",
    "\n",
    "# Flatten interaction matrix and features for neural network input\n",
    "X = spotify_features_scaled.repeat(len(user_profiles), axis=0)\n",
    "y = interaction.flatten()\n",
    "\n",
    "interaction_scores.shape, interaction_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Student\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled.\n",
      "Data split into training and testing sets. Training model...\n",
      "Epoch 1/10\n",
      "\u001b[1m265595/265595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m792s\u001b[0m 3ms/step - accuracy: 0.2497 - loss: -35.8333 - val_accuracy: 0.2506 - val_loss: -35.8444\n",
      "Epoch 2/10\n",
      "\u001b[1m265595/265595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m557s\u001b[0m 2ms/step - accuracy: 0.2497 - loss: -35.9030 - val_accuracy: 0.2506 - val_loss: -35.8444\n",
      "Epoch 3/10\n",
      "\u001b[1m265595/265595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m556s\u001b[0m 2ms/step - accuracy: 0.2499 - loss: -35.8948 - val_accuracy: 0.2506 - val_loss: -35.8444\n",
      "Epoch 4/10\n",
      "\u001b[1m265595/265595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m550s\u001b[0m 2ms/step - accuracy: 0.2499 - loss: -35.8951 - val_accuracy: 0.2506 - val_loss: -35.8444\n",
      "Epoch 5/10\n",
      "\u001b[1m265595/265595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m565s\u001b[0m 2ms/step - accuracy: 0.2499 - loss: -35.8944 - val_accuracy: 0.2506 - val_loss: -35.8444\n",
      "Epoch 6/10\n",
      "\u001b[1m265595/265595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m550s\u001b[0m 2ms/step - accuracy: 0.2500 - loss: -35.8925 - val_accuracy: 0.2506 - val_loss: -35.8444\n",
      "Epoch 7/10\n",
      "\u001b[1m265595/265595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m576s\u001b[0m 2ms/step - accuracy: 0.2499 - loss: -35.8959 - val_accuracy: 0.2506 - val_loss: -35.8444\n",
      "Epoch 8/10\n",
      "\u001b[1m265595/265595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m576s\u001b[0m 2ms/step - accuracy: 0.2500 - loss: -35.8931 - val_accuracy: 0.2506 - val_loss: -35.8444\n",
      "Epoch 9/10\n",
      "\u001b[1m265595/265595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m552s\u001b[0m 2ms/step - accuracy: 0.2496 - loss: -35.9117 - val_accuracy: 0.2506 - val_loss: -35.8444\n",
      "Epoch 10/10\n",
      "\u001b[1m265595/265595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m574s\u001b[0m 2ms/step - accuracy: 0.2498 - loss: -35.8999 - val_accuracy: 0.2506 - val_loss: -35.8444\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Prediction complete and memory cleared.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nNearestNeighbors does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 55\u001b[0m\n\u001b[0;32m     53\u001b[0m features_with_score \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([genre_columns, artist_columns, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_interaction\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[0;32m     54\u001b[0m knn \u001b[38;5;241m=\u001b[39m NearestNeighbors(n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 55\u001b[0m \u001b[43mknn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspotify_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures_with_score\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk-NN model set up.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     58\u001b[0m user_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m  \u001b[38;5;66;03m# Adjust based on user profile index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Student\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Student\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neighbors\\_unsupervised.py:175\u001b[0m, in \u001b[0;36mNearestNeighbors.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# NearestNeighbors.metric is not validated yet\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    157\u001b[0m )\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    159\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the nearest neighbors estimator from the training dataset.\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \n\u001b[0;32m    161\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;124;03m        The fitted nearest neighbors estimator.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Student\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:518\u001b[0m, in \u001b[0;36mNeighborsBase._fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, (KDTree, BallTree, NeighborsBase)):\n\u001b[1;32m--> 518\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_algorithm_metric()\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Student\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\Student\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1049\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1043\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1044\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1045\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1046\u001b[0m     )\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1049\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1057\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1058\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Student\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:126\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Student\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:175\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    174\u001b[0m     )\n\u001b[1;32m--> 175\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nNearestNeighbors does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "# Assuming X, y, and spotify_features_scaled are properly computed as discussed\n",
    "\n",
    "# Custom focal loss implementation\n",
    "def focal_loss(gamma=2., alpha=4.):\n",
    "    gamma = float(gamma)\n",
    "    alpha = float(alpha)\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        alpha_t = y_true*alpha + (tf.keras.backend.ones_like(y_true) - y_true)*(1-alpha)\n",
    "        p_t = y_true*y_pred + (tf.keras.backend.ones_like(y_true) - y_true)*(1-y_pred)\n",
    "        fl = - alpha_t * tf.keras.backend.pow((tf.keras.backend.ones_like(y_true) - p_t), gamma) * tf.keras.backend.log(p_t)\n",
    "        return tf.keras.backend.mean(fl)\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# Neural network setup with Dropout\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_dim=X.shape[1]),\n",
    "    Dropout(0.1),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam', loss=focal_loss(), metrics=['accuracy'])\n",
    "\n",
    "print(\"Model compiled.\")\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Data split into training and testing sets. Training model...\")\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "gc.collect()\n",
    "\n",
    "# Predict interaction scores for all songs using batch prediction\n",
    "predicted_scores = model.predict(spotify_features_scaled).flatten()\n",
    "spotify_data['predicted_interaction'] = predicted_scores\n",
    "gc.collect()  # Clear memory of no longer needed large objects\n",
    "print(\"Prediction complete and memory cleared.\")\n",
    "\n",
    "# k-NN model using enhanced features\n",
    "features_with_score = np.concatenate([genre_columns, artist_columns, ['predicted_interaction']])\n",
    "knn = NearestNeighbors(n_neighbors=5, metric='euclidean')\n",
    "knn.fit(spotify_data[features_with_score])\n",
    "print(\"k-NN model set up.\")\n",
    "\n",
    "user_index = 3  # Adjust based on user profile index\n",
    "user_id = user_profiles.iloc[user_index]['userid']\n",
    "user_top_genre = user_profiles.iloc[user_index]['genre']\n",
    "user_feature_vector = user_features_scaled[user_index].reshape(1, -1)\n",
    "user_predicted_score = model.predict(user_feature_vector).flatten()[0]\n",
    "query_vector = np.append(user_feature_vector, user_predicted_score).reshape(1, -1)\n",
    "\n",
    "# Finding top 5 nearest songs\n",
    "distances, indices = knn.kneighbors(query_vector)\n",
    "recommended_songs = spotify_data.iloc[indices[0]]\n",
    "\n",
    "# Output recommended songs\n",
    "print(f\"Recommended Songs for User: {user_id}, Top Genre: {user_top_genre}\")\n",
    "print(recommended_songs[['track', 'artist', 'genre']])\n",
    "\n",
    "# Validation\n",
    "predicted_interactions = model.predict(X_test).flatten()\n",
    "rmse = mean_squared_error(y_test, predicted_interactions, squared=False)\n",
    "print(\"RMSE for neural network predictions:\", rmse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
